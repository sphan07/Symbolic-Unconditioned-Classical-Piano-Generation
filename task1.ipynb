{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements \n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "\n",
    "from symusic import Score\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from midi2audio import FluidSynth # Import library\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5d4941",
   "metadata": {},
   "source": [
    "Task 1 <br>\n",
    "This assignment focuses on symbolic music modeling. The goal is to train a model that learns a distribution \\( p(x) \\) over symbolic music data (e.g., MIDI)  specifically within the classicalgenre. In addition it is capable of sampling new sequences from this learned distribution unconditionally. We will be using the LSTM model for this task. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a78c33-7dad-46c7-ada7-0faa0a38252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_files = glob.glob(\"./train/*.midi\")\n",
    "test_files = glob.glob(\"./test/*.midi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c6bc2d",
   "metadata": {},
   "source": [
    "LSTM Model<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5965ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=768, hidden_dim=1024, num_layers=4,\n",
    "                 dropout=0.3, bidirectional=False, max_position_embeddings=1024):\n",
    "        super(MusicRNN, self).__init__()\n",
    "        \n",
    "        # Larger embeddings \n",
    "        self.token_embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = torch.nn.Embedding(max_position_embeddings, embedding_dim)\n",
    "        \n",
    "        # Deeper LSTM with residual connections\n",
    "        self.rnn_layers = torch.nn.ModuleList([\n",
    "            torch.nn.LSTM(\n",
    "                input_size=embedding_dim if i == 0 else hidden_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=1,\n",
    "                dropout=0,\n",
    "                batch_first=True,\n",
    "                bidirectional=bidirectional\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        rnn_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        \n",
    "        # Enhanced output processing\n",
    "        self.layer_norms = torch.nn.ModuleList([\n",
    "            torch.nn.LayerNorm(hidden_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # Multi-layer output head\n",
    "        self.output_projection = torch.nn.Sequential(\n",
    "            torch.nn.Linear(rnn_output_dim, rnn_output_dim // 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(rnn_output_dim // 2, vocab_size)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        #Xavier initialization for better training\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, torch.nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "    \n",
    "    def forward(self, x, position_ids, hidden_states=None):\n",
    "        # Token and position embeddings\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(position_ids)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through LSTM layers with residual connections\n",
    "        new_hidden_states = []\n",
    "        for i, (rnn_layer, layer_norm) in enumerate(zip(self.rnn_layers, self.layer_norms)):\n",
    "            hidden = hidden_states[i] if hidden_states else None\n",
    "            out, new_hidden = rnn_layer(x, hidden)\n",
    "            out = layer_norm(out)\n",
    "            \n",
    "            # Residual connection (when dimensions match)\n",
    "            if i > 0 and out.size(-1) == x.size(-1):\n",
    "                out = out + x\n",
    "            \n",
    "            x = self.dropout(out)\n",
    "            new_hidden_states.append(new_hidden)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_projection(x)\n",
    "        return output, new_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba35325-a7b2-4505-8b8d-7d290ae587dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0.001, restore_best_weights=True):\n",
    "        self.patience = patience  # Number of epochs to wait for improvement before stopping\n",
    "        self.min_delta = min_delta # Min change in validation loss to qualify as an improvement\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "\n",
    "        # Internal variables for tracking state\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    # check current loss improved beyond the min_delta threshold\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        # If no improvement then early stopping will be triggered\n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9a95d",
   "metadata": {},
   "source": [
    "Check if gpu is avaliable or else cpu will be used instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca35d6-b6b8-47f0-a170-6b90be8ef2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_setup():\n",
    "    \"\"\"Check and force GPU usage\"\"\"\n",
    "    print(\"=== GPU Setup Check ===\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current CUDA Device: {torch.cuda.current_device()}\")\n",
    "        print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "        device = torch.device('cuda')\n",
    "        print(\"Using GPU\")\n",
    "    else:\n",
    "        print(\"CUDA not available!\")\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Using CPU\")\n",
    "    \n",
    "    print(\"=\" * 30)\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b8fca1",
   "metadata": {},
   "source": [
    "Training<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, vocab_size, num_epochs=50, \n",
    "                        initial_lr=1e-3, device='cpu', save_path='best_model.pth'):\n",
    "    #Advanced training with learning rate scheduling and early stopping\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=initial_lr, weight_decay=0.01)\n",
    "    \n",
    "    #Learning rate scheduler \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
    "    \n",
    "   \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "    \n",
    "    print(f\"Starting training with {len(train_loader)} batches per epoch\")\n",
    "    print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "    print(f\"Training on: {device}\")\n",
    "    \n",
    "    # Training phase\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            batch = batch['input_ids'].to(device)\n",
    "            \n",
    "            if batch.size(1) < 2:\n",
    "                continue\n",
    "                \n",
    "            input_ids = batch[:, :-1]\n",
    "            target_ids = batch[:, 1:]\n",
    "            \n",
    "            position_ids = torch.arange(input_ids.size(1), device=device).unsqueeze(0).expand_as(input_ids)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(input_ids, position_ids)\n",
    "            \n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            targets = target_ids.reshape(-1)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = total_train_loss / num_batches if num_batches > 0 else 0\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch['input_ids'].to(device)\n",
    "                \n",
    "                if batch.size(1) < 2:\n",
    "                    continue\n",
    "                    \n",
    "                input_ids = batch[:, :-1]\n",
    "                target_ids = batch[:, 1:]\n",
    "                position_ids = torch.arange(input_ids.size(1), device=device).unsqueeze(0).expand_as(input_ids)\n",
    "                \n",
    "                outputs, _ = model(input_ids, position_ids)\n",
    "                outputs = outputs.reshape(-1, vocab_size)\n",
    "                targets = target_ids.reshape(-1)\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = total_val_loss / val_batches if val_batches > 0 else 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Record metrics\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        learning_rates.append(current_lr)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {current_lr:.2e}\")\n",
    "        \n",
    "        # GPU memory info \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB allocated\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping(avg_val_loss, model):\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'vocab_size': vocab_size\n",
    "            }, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'vocab_size': vocab_size,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'learning_rates': learning_rates\n",
    "    }, save_path)\n",
    "    \n",
    "    return train_losses, val_losses, learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcbb421-711f-41f4-b07d-0376905601e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training():\n",
    "    #Set up and run the improved training pipeline\n",
    "    \n",
    "    # Force GPU check first\n",
    "    device = check_gpu_setup()\n",
    "    \n",
    "    # Create tokenizer\n",
    "    config = TokenizerConfig(\n",
    "        num_velocities=32,           \n",
    "        use_chords=True,            \n",
    "        use_programs=False,         \n",
    "        use_time_signatures=True,   \n",
    "        use_rests=True, \n",
    "        use_tempos=True            \n",
    "    )\n",
    "    tokenizer = REMI(config)\n",
    "    tokenizer.train(vocab_size=1500, files_paths=train_files)\n",
    "    tokenizer.save(\"tokenizer.json\")\n",
    "    print(f\"Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = DatasetMIDI(\n",
    "        files_paths=train_files,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_len=1024,\n",
    "        bos_token_id=tokenizer[\"BOS_None\"],\n",
    "        eos_token_id=tokenizer[\"EOS_None\"],\n",
    "    )\n",
    "    \n",
    "    test_dataset = DatasetMIDI(\n",
    "        files_paths=test_files,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_len=1024,\n",
    "        bos_token_id=tokenizer[\"BOS_None\"],\n",
    "        eos_token_id=tokenizer[\"EOS_None\"],\n",
    "    )\n",
    "    \n",
    "    collator = DataCollator(tokenizer.pad_token_id)\n",
    "    # Smaller batch size for GPU memory\n",
    "    batch_size = 16 if device.type == 'cuda' else 8\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collator)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collator)\n",
    "    \n",
    "    model = MusicRNN(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        embedding_dim=768,\n",
    "        hidden_dim=1024,\n",
    "        num_layers=4,\n",
    "        dropout=0.3,\n",
    "        max_position_embeddings=1024\n",
    "    )\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    train_losses, val_losses, learning_rates = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=test_loader,\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        num_epochs=50,\n",
    "        initial_lr=1e-3,\n",
    "        device=device,  \n",
    "        save_path='improved_music_model.pth'\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer, train_losses, val_losses, learning_rates\n",
    "\n",
    "# Run the improved training\n",
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer, train_losses, val_losses, learning_rates = setup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7812bc-6d24-4fd2-813d-1459ba3e69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model_fixed():\n",
    "    # model loading function\n",
    "    try:\n",
    "        model_path = 'improved_music_model.pth'\n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        \n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\" Model file {model_path} not found!\")\n",
    "            return None, None\n",
    "            \n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "        \n",
    "        # Load tokenizer \n",
    "        tokenizer = None\n",
    "        if os.path.exists(\"tokenizer.json\"):\n",
    "            print(\"Loading existing tokenizer...\")\n",
    "            try:\n",
    "                # Correct way to load REMI tokenizer\n",
    "                tokenizer = REMI.from_pretrained(\".\")  # Load from current directory\n",
    "                print(\" Loaded existing tokenizer\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load existing tokenizer: {e}\")\n",
    "                tokenizer = None\n",
    "        \n",
    "        if tokenizer is None:\n",
    "            print(\"Creating new tokenizer...\")\n",
    "            # Get training files\n",
    "            train_files = glob.glob(\"./train/*.midi\")\n",
    "            if not train_files:\n",
    "                print(\"No training files found in ./train/*.midi\")\n",
    "                return None, None\n",
    "                \n",
    "            config = TokenizerConfig(\n",
    "                num_velocities=32,           \n",
    "                use_chords=True,            \n",
    "                use_programs=False,         \n",
    "                use_time_signatures=True,   \n",
    "                use_rests=True, \n",
    "                use_tempos=True            \n",
    "            )\n",
    "            tokenizer = REMI(config)\n",
    "            tokenizer.train(vocab_size=1500, files_paths=train_files)\n",
    "            tokenizer.save(\"tokenizer.json\")\n",
    "            print(\" Created and saved new tokenizer\")\n",
    "        \n",
    "        # Create model with same architecture\n",
    "        vocab_size = checkpoint.get('vocab_size', tokenizer.vocab_size)\n",
    "        model = MusicRNN(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=768,\n",
    "            hidden_dim=1024,\n",
    "            num_layers=4,\n",
    "            dropout=0.3,\n",
    "            max_position_embeddings=1024\n",
    "        )\n",
    "        \n",
    "        # Load the trained weights\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\" Successfully loaded your trained model!\")\n",
    "        \n",
    "        # Show training info if available\n",
    "        if 'train_losses' in checkpoint:\n",
    "            print(f\"Model was trained for {len(checkpoint['train_losses'])} epochs\")\n",
    "            print(f\"Final training loss: {checkpoint['train_losses'][-1]:.4f}\")\n",
    "        if 'val_losses' in checkpoint:\n",
    "            print(f\"Final validation loss: {checkpoint['val_losses'][-1]:.4f}\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "def plot_training_progress(train_losses, val_losses, learning_rates):\n",
    "    #Plot training metrics to visualize progress\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(train_losses, label='Training Loss', color='blue')\n",
    "    axes[0].plot(val_losses, label='Validation Loss', color='red')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training Progress')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Learning rate plot\n",
    "    axes[1].plot(learning_rates, color='green')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Learning Rate')\n",
    "    axes[1].set_title('Learning Rate Schedule')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Plot results if matplotlib is available\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plot_training_progress(train_losses, val_losses, learning_rates)\n",
    "    except ImportError:\n",
    "        print(\"Install matplotlib to see training plots: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d44b8c",
   "metadata": {},
   "source": [
    "Functions to improve the musicality of the midi file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9428849b-62d7-4536-bd2b-539d6d114ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_string(tokenizer, token_id):\n",
    "    # Helper function to get token string\n",
    "    try:\n",
    "        # Access token vocabulary\n",
    "        if hasattr(tokenizer, '_vocab_base'):\n",
    "            for name, idx in tokenizer._vocab_base.items():\n",
    "                if idx == token_id:\n",
    "                    return name\n",
    "        elif hasattr(tokenizer, 'vocab'):\n",
    "            if token_id < len(tokenizer.vocab):\n",
    "                return tokenizer.vocab[token_id]\n",
    "        elif hasattr(tokenizer, '_vocab'):\n",
    "            for name, idx in tokenizer._vocab.items():\n",
    "                if idx == token_id:\n",
    "                    return name\n",
    "        return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddad66f-9993-4708-8305-618919bc88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_musical_context(tokenizer, recent_tokens, max_lookback=20):\n",
    "    \n",
    "    #Analyze recent tokens to understand current musical context (Returns tempo, chord, and structural information)\n",
    "    \n",
    "    context = {\n",
    "        'current_tempo': None,\n",
    "        'current_chord': None,\n",
    "        'recent_pitches': [],\n",
    "        'current_velocity': None,\n",
    "        'bar_position': 0,\n",
    "        'time_signature': None\n",
    "    }\n",
    "    \n",
    "    # Look at recent tokens for context\n",
    "    lookback_tokens = recent_tokens[-max_lookback:] if len(recent_tokens) > max_lookback else recent_tokens\n",
    "    \n",
    "    for token in lookback_tokens:\n",
    "        token_str = get_token_string(tokenizer, token)\n",
    "        if not token_str:\n",
    "            continue\n",
    "            \n",
    "        # Extract tempo \n",
    "        if token_str.startswith(\"Tempo_\"):\n",
    "            try:\n",
    "                context['current_tempo'] = int(token_str.split(\"_\")[1])\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # Extract chord \n",
    "        elif token_str.startswith(\"Chord_\"):\n",
    "            context['current_chord'] = token_str\n",
    "            \n",
    "        # Extract pitch info for harmony\n",
    "        elif token_str.startswith(\"Pitch_\"):\n",
    "            try:\n",
    "                pitch = int(token_str.split(\"_\")[1])\n",
    "                context['recent_pitches'].append(pitch)\n",
    "                # Keep recent pitches\n",
    "                context['recent_pitches'] = context['recent_pitches'][-8:]\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # Extract velocity for dynamics\n",
    "        elif token_str.startswith(\"Velocity_\"):\n",
    "            try:\n",
    "                context['current_velocity'] = int(token_str.split(\"_\")[1])\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # Extract time signature\n",
    "        elif token_str.startswith(\"TimeSig_\"):\n",
    "            context['time_signature'] = token_str\n",
    "            \n",
    "        # Track bar position\n",
    "        elif token_str.startswith(\"Position_\"):\n",
    "            try:\n",
    "                context['bar_position'] = int(token_str.split(\"_\")[1])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b7a56-6e5a-44c3-9402-6c088fa6f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_musical_biasing(logits, tokenizer, context, step, bias_strength=2.0):\n",
    "   \n",
    "    #apply musical knowledge to bias the probability distribution\n",
    "\n",
    "    vocab_size = logits.shape[-1]\n",
    "    bias = torch.zeros_like(logits)\n",
    "    \n",
    "    # Define common chord progressions and musical patterns\n",
    "    common_tempos = [60, 72, 80, 90, 100, 110, 120, 132, 144]  # Common classical tempos\n",
    "    consonant_intervals = [0, 3, 4, 7, 12]  # Unison, minor 3rd, major 3rd, perfect 5th, octave\n",
    "    \n",
    "    for token_id in range(min(vocab_size, tokenizer.vocab_size)):\n",
    "        token_str = get_token_string(tokenizer, token_id)\n",
    "        if not token_str:\n",
    "            continue\n",
    "            \n",
    "        # Tempo biasing - favor stable, musical tempos\n",
    "        if token_str.startswith(\"Tempo_\"):\n",
    "            try:\n",
    "                tempo = int(token_str.split(\"_\")[1])\n",
    "                if tempo in common_tempos:\n",
    "                    bias[0, token_id] += bias_strength * 0.3\n",
    "                elif 60 <= tempo <= 160:  # Reasonable tempo range\n",
    "                    bias[0, token_id] += bias_strength * 0.1\n",
    "                else:\n",
    "                    bias[0, token_id] -= bias_strength * 0.2  # Discourage extreme tempos\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # Chord progression biasing\n",
    "        elif token_str.startswith(\"Chord_\"):\n",
    "            # Favor common chord types\n",
    "            if any(chord_type in token_str for chord_type in [\"_M\", \"_m\", \"_dim\", \"_7\"]):\n",
    "                bias[0, token_id] += bias_strength * 0.2\n",
    "                \n",
    "        # Harmonic biasing for pitches\n",
    "        elif token_str.startswith(\"Pitch_\"):\n",
    "            try:\n",
    "                pitch = int(token_str.split(\"_\")[1])\n",
    "                \n",
    "                # If we have recent pitches, favor consonant intervals\n",
    "                if context['recent_pitches']:\n",
    "                    for recent_pitch in context['recent_pitches'][-3:]:  # Check last 3 pitches\n",
    "                        interval = abs(pitch - recent_pitch) % 12\n",
    "                        if interval in consonant_intervals:\n",
    "                            bias[0, token_id] += bias_strength * 0.15\n",
    "                        elif interval in [1, 2, 10, 11]:  # Dissonant intervals\n",
    "                            bias[0, token_id] -= bias_strength * 0.1\n",
    "                            \n",
    "                # Favor pitches in reasonable range (piano range roughly)\n",
    "                if 21 <= pitch <= 108:  # A0 to C8\n",
    "                    bias[0, token_id] += bias_strength * 0.05\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # Velocity biasing for natural dynamics\n",
    "        elif token_str.startswith(\"Velocity_\"):\n",
    "            try:\n",
    "                velocity = int(token_str.split(\"_\")[1])\n",
    "                # Favor moderate velocities, avoid extremes\n",
    "                if 40 <= velocity <= 100:\n",
    "                    bias[0, token_id] += bias_strength * 0.1\n",
    "                elif velocity < 20 or velocity > 120:\n",
    "                    bias[0, token_id] -= bias_strength * 0.15\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # Structural biasing\n",
    "        elif token_str.startswith(\"Position_\"):\n",
    "            try:\n",
    "                position = int(token_str.split(\"_\")[1])\n",
    "                # Favor positions that align with musical structure (beats)\n",
    "                if position % 24 == 0:  # Strong beats (assuming 24 ticks per quarter)\n",
    "                    bias[0, token_id] += bias_strength * 0.1\n",
    "                elif position % 12 == 0:  # Half beats\n",
    "                    bias[0, token_id] += bias_strength * 0.05\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1946e3-556b-4c81-a267-c93fec7a63b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encourage_musical_structure(logits, tokenizer, context, step, structure_strength=1.5):\n",
    "   \n",
    "    #encourage musical structure like proper phrase lengths and cadences into midi\n",
    "\n",
    "    bias = torch.zeros_like(logits)\n",
    "    vocab_size = logits.shape[-1]\n",
    "    \n",
    "    #encourage structural elements\n",
    "    if step % 64 < 8:  \n",
    "        # Encourage new chord/tempo \n",
    "        for token_id in range(min(vocab_size, tokenizer.vocab_size)):\n",
    "            token_str = get_token_string(tokenizer, token_id)\n",
    "            if token_str and (token_str.startswith(\"Chord_\") or token_str.startswith(\"Tempo_\")):\n",
    "                bias[0, token_id] += structure_strength * 0.2\n",
    "                \n",
    "    elif step % 64 > 56:  # End of phrase\n",
    "        # Encourage cadential patterns like chord changes & rests\n",
    "        for token_id in range(min(vocab_size, tokenizer.vocab_size)):\n",
    "            token_str = get_token_string(tokenizer, token_id)\n",
    "            if token_str and (token_str.startswith(\"Chord_\") or token_str.startswith(\"Rest_\")):\n",
    "                bias[0, token_id] += structure_strength * 0.3\n",
    "    \n",
    "    return bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e346c52",
   "metadata": {},
   "source": [
    "Sampling using funcitons from above<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_structure(\n",
    "    model, tokenizer, start_token, max_length=2048,\n",
    "    temperature=1.0, top_k=10, top_p=0.9,\n",
    "    device='cuda', musical_bias=True, bias_strength=2.0\n",
    "):\n",
    "   \n",
    "    #sampling function with musical knowledge and structure awareness \n",
    "    \n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model or tokenizer is None\")\n",
    "        return []\n",
    "        \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    generated = [start_token]\n",
    "    input_token = torch.tensor([[start_token]], device=device)\n",
    "    input_pos = torch.tensor([[0]], device=device)\n",
    "    hidden = None\n",
    "    current_position = 0\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    print(f\"Starting enhanced sampling with vocab_size: {vocab_size}\")\n",
    "    print(f\"Musical biasing: {'enabled' if musical_bias else 'disabled'}\")\n",
    "\n",
    "    for step in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                output, hidden = model(input_token, input_pos, hidden)\n",
    "                logits = output[:, -1, :] / temperature\n",
    "                logits = logits[:, :vocab_size]  # Constrain to valid vocab\n",
    "\n",
    "                # Apply musical biasing if enabled\n",
    "                if musical_bias:\n",
    "                    context = get_musical_context(tokenizer, generated)\n",
    "                    \n",
    "                    # Apply musical knowledge biasing\n",
    "                    musical_bias_tensor = apply_musical_biasing(\n",
    "                        logits, tokenizer, context, step, bias_strength\n",
    "                    )\n",
    "                    \n",
    "                    # Apply structural biasing\n",
    "                    structure_bias = encourage_musical_structure(\n",
    "                        logits, tokenizer, context, step, bias_strength * 0.75\n",
    "                    )\n",
    "                    \n",
    "                    # Combine biases\n",
    "                    logits = logits + musical_bias_tensor + structure_bias\n",
    "\n",
    "                # Top-p (nucleus) sampling\n",
    "                if top_p < 1.0:\n",
    "                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    \n",
    "                    # Create mask for tokens to remove\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = 0  # Never remove the top token\n",
    "                    \n",
    "                    # Apply mask\n",
    "                    for b in range(logits.shape[0]):\n",
    "                        logits[b, sorted_indices[b][sorted_indices_to_remove[b]]] = -float(\"Inf\")\n",
    "\n",
    "                # Top-k sampling\n",
    "                if top_k > 0:\n",
    "                    top_k_values, _ = torch.topk(logits, min(top_k, vocab_size))\n",
    "                    min_top_k = top_k_values[:, -1].unsqueeze(-1)\n",
    "                    logits[logits < min_top_k] = -float(\"Inf\")\n",
    "\n",
    "                # Sample from the filtered distribution\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                # Safety check for out-of-vocabulary tokens\n",
    "                if next_token >= vocab_size:\n",
    "                    try:\n",
    "                        next_token = tokenizer[\"EOS_None\"]\n",
    "                    except:\n",
    "                        next_token = 0  # Fallback to pad token\n",
    "                        \n",
    "                generated.append(next_token)\n",
    "\n",
    "                # Get token string for position and structure logic\n",
    "                token_str = get_token_string(tokenizer, next_token)\n",
    "\n",
    "                # Update position with musical structure awareness\n",
    "                if token_str and token_str.startswith(\"Position_\"):\n",
    "                    try:\n",
    "                        current_position = int(token_str.split(\"_\")[1])\n",
    "                        current_position = min(current_position, 95)  # Cap at 95\n",
    "                    except:\n",
    "                        current_position = (current_position + 1) % 96\n",
    "                elif token_str and token_str.startswith(\"Bar\"):\n",
    "                    # Reset position at bar boundaries for musical structure\n",
    "                    current_position = 0\n",
    "                else:\n",
    "                    # Default increment with wraparound\n",
    "                    current_position = (current_position + 1) % 96\n",
    "\n",
    "                # Check for end tokens\n",
    "                try:\n",
    "                    eos_token = tokenizer[\"EOS_None\"]\n",
    "                    pad_token = tokenizer.get(\"PAD_None\", 0)\n",
    "                    if next_token in [eos_token, pad_token]:\n",
    "                        break\n",
    "                except:\n",
    "                    # If we can't access special tokens, continue\n",
    "                    pass\n",
    "\n",
    "                # Prepare for next iteration\n",
    "                input_token = torch.tensor([[next_token]], device=device)\n",
    "                input_pos = torch.tensor([[current_position]], device=device)\n",
    "                \n",
    "                # Print progress occasionally with musical context\n",
    "                if step % 100 == 0:\n",
    "                    context = get_musical_context(tokenizer, generated)\n",
    "                    print(f\"Step {step}: Generated {len(generated)} tokens, \"\n",
    "                          f\"Tempo: {context['current_tempo']}, \"\n",
    "                          f\"Chord: {context['current_chord']}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during sampling at step {step}: {e}\")\n",
    "                break\n",
    "\n",
    "    print(f\"Generated {len(generated)} tokens total\")\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec2585a",
   "metadata": {},
   "source": [
    "Generation output of midi files<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_midi(tokenizer, generated_sequence, output_filename=\"rnn.mid\"):\n",
    "    #MIDI generation function\n",
    "    try:\n",
    "        if not generated_sequence:\n",
    "            print(\"empty generated sequence\")\n",
    "            return None\n",
    "            \n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        valid_sequence = [token for token in generated_sequence if 0 <= token < vocab_size]\n",
    "        \n",
    "        print(f\"Original sequence length: {len(generated_sequence)}\")\n",
    "        print(f\"Valid sequence length: {len(valid_sequence)}\")\n",
    "        \n",
    "        if len(valid_sequence) < 2:\n",
    "            print(\"sequence too short or no valid tokens found\")\n",
    "            return None\n",
    "            \n",
    "        # Decode tokens to MIDI\n",
    "        try:\n",
    "            output_scores = tokenizer.decode([valid_sequence])\n",
    "            \n",
    "            if isinstance(output_scores, list):\n",
    "                output_score = output_scores[0]\n",
    "            else:\n",
    "                output_score = output_scores\n",
    "            \n",
    "            if len(output_score.tracks) == 0:\n",
    "                print(\"MIDI generated has no tracks\")\n",
    "                return None\n",
    "                \n",
    "            output_score.dump_midi(output_filename)\n",
    "            print(f\"successfully generated {output_filename}\")\n",
    "            return output_score\n",
    "            \n",
    "        except Exception as decode_error:\n",
    "            print(f\"Error during token decoding: {decode_error}\")\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during MIDI generation: {e}\")\n",
    "        print(f\"Sequence sample: {generated_sequence[:20] if generated_sequence else 'Empty'}...\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879bf97a-e88e-41a6-96e4-90bf6a39c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_generation():\n",
    "    \n",
    "    #function to enhanced the generation with existing model\n",
    "    \n",
    "    print(\"=== Testing Enhanced Generation ===\")\n",
    "    \n",
    "    # Load existing trained model\n",
    "    model, tokenizer = load_trained_model_fixed()\n",
    "    \n",
    "    if model is None or tokenizer is None:\n",
    "        print(\" model and tokenizer load failed\")\n",
    "        return\n",
    "    \n",
    "    print(f\" Model loaded successfully\")\n",
    "    print(f\"Tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    try:\n",
    "        start_token = tokenizer[\"BOS_None\"]\n",
    "    except:\n",
    "        print(\"Using fallback start token\")\n",
    "        start_token = 1\n",
    "    \n",
    "    print(\"\\n=== Generating with Musical Enhancements ===\")\n",
    "    \n",
    "    # Generate with enhanced musical biasing\n",
    "    generated_sequence = sample_with_structure(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        max_length=1024,       \n",
    "        temperature=0.825,    \n",
    "        top_k=20,           \n",
    "        top_p=0.95,         \n",
    "        device=device,\n",
    "        musical_bias=True,    \n",
    "        bias_strength=2.0     \n",
    "    )\n",
    "    \n",
    "    if not generated_sequence:\n",
    "        print(\" Generation failed\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Generated sequence of length {len(generated_sequence)}\")\n",
    "    \n",
    "    # Generate MIDI\n",
    "    print(\"\\n=== Creating Enhanced MIDI ===\")\n",
    "    output_score = generate_midi(tokenizer, generated_sequence, \"enhanced_music2.mid\")\n",
    "    \n",
    "    if output_score:\n",
    "        print(\"midi generated\")\n",
    "        print(\"File saved as: enhanced_music.mid\")\n",
    "        \n",
    "        # Show musical analysis\n",
    "        context = get_musical_context(tokenizer, generated_sequence, max_lookback=len(generated_sequence))\n",
    "        print(f\"\\n=== Musical Analysis ===\")\n",
    "        print(f\"Final tempo: {context['current_tempo']}\")\n",
    "        print(f\"Final chord: {context['current_chord']}\")\n",
    "        print(f\"Unique pitches used: {len(set(context['recent_pitches']))}\")\n",
    "        print(f\"Final velocity: {context['current_velocity']}\")\n",
    "    else:\n",
    "        print(\"MIDI generation failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d3356-f673-4c9b-89a5-6ebf146a8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_generation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
