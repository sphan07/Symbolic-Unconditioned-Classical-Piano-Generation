{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecb548",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'nn' from partially initialized module 'torch' (most likely due to a circular import) (C:\\Users\\sammy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py:2108\u001b[39m\n\u001b[32m   2101\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2103\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2104\u001b[39m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[32m   2105\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2106\u001b[39m \n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2108\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF \u001b[38;5;28;01mas\u001b[39;00m _VF, functional \u001b[38;5;28;01mas\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2109\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[32m   2111\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2112\u001b[39m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[32m   2113\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\functional.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional, Sequence, Tuple, TYPE_CHECKING, Union\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF, Tensor\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'nn' from partially initialized module 'torch' (most likely due to a circular import) (C:\\Users\\sammy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py)"
     ]
    }
   ],
   "source": [
    "#sampling function\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def sample_with_structure(\n",
    "    model, tokenizer, start_token, max_length=1024,\n",
    "    temperature=1.0, top_k=10, top_p=0.9,\n",
    "    device='cuda'\n",
    "):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    generated = [start_token]\n",
    "    input_token = torch.tensor([[start_token]], device=device)\n",
    "    input_pos = torch.tensor([[0]], device=device)\n",
    "    hidden = None\n",
    "    current_position = 0\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    for step in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_token, input_pos, hidden)\n",
    "            logits = output[:, -1, :] / temperature\n",
    "            logits = logits[:, :vocab_size]\n",
    "\n",
    "            # Top-k and Top-p (nucleus) sampling\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "            # Nucleus sampling: remove tokens with cumulative prob above top_p\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            for b in range(logits.shape[0]):\n",
    "                logits[b, sorted_indices[b][sorted_indices_to_remove[b]]] = -float(\"Inf\")\n",
    "\n",
    "            # Top-k sampling: keep only top-k tokens\n",
    "            if top_k > 0:\n",
    "                top_k_values, _ = torch.topk(logits, top_k)\n",
    "                min_top_k = top_k_values[:, -1].unsqueeze(-1)\n",
    "                logits[logits < min_top_k] = -float(\"Inf\")\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated.append(next_token)\n",
    "\n",
    "            # Token string (for position and bar logic)\n",
    "            token_str = None\n",
    "            for name, idx in tokenizer._vocab_base.items():\n",
    "                if idx == next_token:\n",
    "                    token_str = name\n",
    "                    break\n",
    "\n",
    "            # Update position â€” capped to max 95\n",
    "            if token_str and token_str.startswith(\"Position_\"):\n",
    "                try:\n",
    "                    current_position = int(token_str.split(\"_\")[1])\n",
    "                except:\n",
    "                    current_position = (current_position + 1) % 96\n",
    "            else:\n",
    "                current_position = (current_position + 1) % 96\n",
    "\n",
    "            # Reset position if Bar or EOS is predicted\n",
    "            if token_str and token_str.startswith(\"Bar\"):\n",
    "                current_position = 0\n",
    "\n",
    "            if next_token in [tokenizer[\"EOS_None\"], tokenizer[\"PAD_None\"]]:\n",
    "                break\n",
    "\n",
    "            input_token = torch.tensor([[next_token]], device=device)\n",
    "            input_pos = torch.tensor([[current_position]], device=device)\n",
    "\n",
    "    return generated\n",
    "\n",
    "start_token = tokenizer[\"BOS_None\"]\n",
    "generated_sequence = sample_with_structure(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    start_token=start_token,\n",
    "    max_length=1024,\n",
    "    temperature=0.9,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d0bf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pretty_midi'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Add Chord Annotations to Your Training Data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpretty_midi\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_chords\u001b[39m(pm, beat_times):\n\u001b[32m      5\u001b[39m     chords = []\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pretty_midi'"
     ]
    }
   ],
   "source": [
    "# Add Chord Annotations to Your Training Data\n",
    "\n",
    "import pretty_midi\n",
    "\n",
    "def add_chord_markers_to_midi(midi_path, out_path, chords):\n",
    "    pm = pretty_midi.PrettyMIDI(midi_path)\n",
    "    for chord_time, chord_label in chords:\n",
    "        pm.markers.append(pretty_midi.Marker(chord_label, chord_time))\n",
    "    pm.write(out_path)\n",
    "\n",
    "\n",
    "import pretty_midi\n",
    "\n",
    "def extract_chords(pm, beat_times):\n",
    "    chords = []\n",
    "    for i in range(len(beat_times) - 1):\n",
    "        start = beat_times[i]\n",
    "        end = beat_times[i + 1]\n",
    "        notes = [note.pitch for inst in pm.instruments for note in inst.notes if start <= note.start < end]\n",
    "        if notes:\n",
    "            root = pretty_midi.note_number_to_name(min(notes))  # crude root guess\n",
    "            chord_label = f\"Chord_{root}maj\"  # placeholder, better use chord recognition lib\n",
    "        else:\n",
    "            chord_label = \"Chord_None\"\n",
    "        chords.append((start, chord_label))\n",
    "    return chords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf58110d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed728b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
